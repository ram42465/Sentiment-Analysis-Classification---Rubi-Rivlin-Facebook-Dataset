{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13e6d677",
   "metadata": {},
   "source": [
    "## Goal:\n",
    "Classify Positive/Negative Sentiment from comments in Ruby Rivlin Facebook Page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23113452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ramirc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import re  \n",
    "import nltk \n",
    "import codecs\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import csv\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from google_trans_new import google_translator  \n",
    "nltk.download('stopwords')  \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn import naive_bayes, svm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeba3db",
   "metadata": {},
   "source": [
    "## Load data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72e6143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#token - Original data\n",
    "token_train = pd.read_csv(\"data/token_train.tsv\", sep='\\t', quoting=csv.QUOTE_NONE, header=None)\n",
    "token_test = pd.read_csv(\"data/token_test.tsv\", sep='\\t', quoting=csv.QUOTE_NONE, header=None)\n",
    "\n",
    "#morph - Data after morphologic processing\n",
    "morph_train = pd.read_csv(\"data/morph_train.tsv\", sep='\\t', quoting=csv.QUOTE_NONE, header=None)\n",
    "morph_test = pd.read_csv(\"data/morph_test.tsv\", sep='\\t', quoting=csv.QUOTE_NONE, header=None)\n",
    "\n",
    "#token_eng - Data after english translation (using google translate)\n",
    "token_train_eng = pd.read_csv(\"data/token_train_english.csv\", header=None)\n",
    "token_test_eng = pd.read_csv(\"data/token_test_english.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebc898ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10244, 2)\n",
      "(10244, 2)\n",
      "(10244, 2)\n"
     ]
    }
   ],
   "source": [
    "print(token_train.shape)\n",
    "print(morph_train.shape)\n",
    "print(token_train_eng.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce59a494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ממש כואב ..... אני בוכה עם המשפחה שלא תדעו עוד...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>איש יקר שלנו</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>כל הכבוד והמון בהצלחה</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\" תל חי , רובי . בכל העצב הזה היית קרן אור של ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>נקי כפיים ובר לבב בהצלחה לך ולנו .</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0  ממש כואב ..... אני בוכה עם המשפחה שלא תדעו עוד...  0\n",
       "1                                       איש יקר שלנו  0\n",
       "2                              כל הכבוד והמון בהצלחה  0\n",
       "3  \" תל חי , רובי . בכל העצב הזה היית קרן אור של ...  0\n",
       "4                 נקי כפיים ובר לבב בהצלחה לך ולנו .  0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e2aa9d",
   "metadata": {},
   "source": [
    "## Plot train data sentiments distribution\n",
    "We can see that most of the comments are possitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f135aa7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(x=1, data=token_train);\n",
    "ax.set(xlabel='Sentiment', ylabel='Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb639e1a",
   "metadata": {},
   "source": [
    "## FIRST MODEL\n",
    "Using Random Forest classifier based on TD-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8e71ec",
   "metadata": {},
   "source": [
    "## Prepare the data for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6e473df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split all data sets to the comments(X) and the sentimnet(y) \n",
    "\n",
    "\n",
    "X_token_train = token_train.iloc[:, 0].values\n",
    "y_token_train = token_train.iloc[:, 1].values\n",
    "X_token_test = token_test.iloc[:, 0].values\n",
    "y_token_test = token_test.iloc[:, 1].values\n",
    "\n",
    "X_morph_train = morph_train.iloc[:, 0].values\n",
    "y_morph_train = morph_train.iloc[:, 1].values\n",
    "X_morph_test = morph_test.iloc[:, 0].values\n",
    "y_morph_test = morph_test.iloc[:, 1].values\n",
    "\n",
    "X_token_train_eng = token_train_eng.iloc[:, 0].values\n",
    "y_token_train_eng = token_train_eng.iloc[:, 1].values\n",
    "X_token_test_eng = token_test_eng.iloc[:, 0].values\n",
    "y_token_test_eng = token_test_eng.iloc[:, 1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52f01cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#proccess_text function will get text array and clean it with regex expressions\n",
    "def process_text(arr, lang):\n",
    "    processed_arr = []\n",
    "    \n",
    "    if lang==\"he\":\n",
    "        for text in range(0, len(arr)):  \n",
    "            processed_text = re.sub(r'[\\a-z]+', ' ', str(arr[text]))\n",
    "            processed_arr.append(processed_text)\n",
    "            \n",
    "    elif lang==\"eng\":\n",
    "        for text in range(0, len(arr)):  \n",
    "            # Remove all the special characters\n",
    "            processed_text = re.sub(r'\\W', ' ', str(arr[text]))\n",
    "\n",
    "            # Remove all digits\n",
    "            #processed_text = re.sub(r'\\d+', ' ', processed_text)\n",
    "\n",
    "            # remove all single characters\n",
    "            processed_text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_text)\n",
    "\n",
    "            # Remove single characters from the start\n",
    "            processed_text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_text) \n",
    "\n",
    "            # Substituting multiple spaces with single space\n",
    "            processed_text= re.sub(r'\\s+', ' ', processed_text, flags=re.I)\n",
    "\n",
    "            # Removing prefixed 'b'\n",
    "            processed_text = re.sub(r'^b\\s+', '', processed_text)\n",
    "\n",
    "            # Converting to Lowercase\n",
    "            processed_text = processed_text.lower()\n",
    "            processed_arr.append(processed_text)\n",
    "    \n",
    "    return processed_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eef9c4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def translate_text(arr):\\n    translator = google_translator()  \\n    english_list = []\\n    for text in arr:\\n        translated_text = translator.translate(text,lang_src='he',lang_tgt='en')  \\n        english_list.append(translated_text)\\n    return np.array(english_list)\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Translate text using google translate API (not in use, i translated the data file outside the notebook)\n",
    "'''def translate_text(arr):\n",
    "    translator = google_translator()  \n",
    "    english_list = []\n",
    "    for text in arr:\n",
    "        translated_text = translator.translate(text,lang_src='he',lang_tgt='en')  \n",
    "        english_list.append(translated_text)\n",
    "    return np.array(english_list)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23bb70a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean Datasets\n",
    "\n",
    "X_token_train_processed = process_text(X_token_train,\"he\")\n",
    "X_token_test_processed = process_text(X_token_test,\"he\")\n",
    "\n",
    "X_morph_train_processed = process_text(X_morph_train,\"he\")\n",
    "X_morph_test_processed = process_text(X_morph_test,\"he\")\n",
    "\n",
    "X_token_train_eng_processed = process_text(X_token_train_eng,\"eng\")\n",
    "X_token_test_eng_processed = process_text(X_token_test_eng,\"eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34c8af8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it really hurts cry with the family that you will not know any more grief',\n",
       " 'our dear man',\n",
       " 'well done and good luck',\n",
       " 'tel hai ruby in all this sadness you were ray of light of hope there is indeed president in israel',\n",
       " 'clean hands and hearty good luck to you and us ']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_token_train_eng_processed[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8a7c3",
   "metadata": {},
   "source": [
    "Create feature vectors containing TF-IDF values\n",
    "- max_features - The number of words that we will create feature for.\n",
    "- min_df - Specify that a word should be occur at least 5 times to treat as feature\n",
    "- max_df = Specify that a word will not occur in more of 70% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "169962c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_token_converter = TfidfVectorizer(max_features=3000, min_df=5, max_df=0.7)  \n",
    "\n",
    "tfidf_morph_converter = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.7)  \n",
    "\n",
    "tfidf_token_eng_converter = TfidfVectorizer(max_features=2000, min_df=5, max_df=0.7)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0c4ec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will use only the train data to fit (create the features) and than we will transform both train and dataset.\n",
    "\n",
    "tfidf_token_converter.fit(X_token_train_processed)\n",
    "X_token_train = tfidf_token_converter.transform(X_token_train_processed).toarray()\n",
    "X_token_test = tfidf_token_converter.transform(X_token_test_processed).toarray()\n",
    "\n",
    "tfidf_morph_converter.fit(X_morph_train_processed)\n",
    "X_morph_train = tfidf_morph_converter.transform(X_morph_train_processed).toarray()\n",
    "X_morph_test = tfidf_morph_converter.transform(X_morph_test_processed).toarray()\n",
    "\n",
    "tfidf_token_eng_converter.fit(X_token_train_eng_processed)\n",
    "X_token_train_eng = tfidf_token_eng_converter.transform(X_token_train_eng_processed).toarray()\n",
    "X_token_test_eng = tfidf_token_eng_converter.transform(X_token_test_eng_processed).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b08c3c4",
   "metadata": {},
   "source": [
    "## Build Classifier\n",
    "I will use random forest to classify the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4a7146e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_classifier = RandomForestClassifier(n_estimators=100, random_state=0)  \n",
    "token_classifier.fit(X_token_train, y_token_train)\n",
    "\n",
    "morph_classifier = RandomForestClassifier(n_estimators=100, random_state=0)  \n",
    "morph_classifier.fit(X_morph_train, y_morph_train)\n",
    "\n",
    "token_eng_classifier = RandomForestClassifier(n_estimators=100, random_state=0)  \n",
    "token_eng_classifier.fit(X_token_train_eng, y_token_train_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c703f14",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff811ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_predictions = token_classifier.predict(X_token_test)\n",
    "morph_predictions = morph_classifier.predict(X_morph_test)\n",
    "token_eng_predictions = token_eng_classifier.predict(X_token_test_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d0dc9b",
   "metadata": {},
   "source": [
    "We can see that we get similiar results in all datasets ~92% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "477a2736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token model\n",
      "[[1622   74    2]\n",
      " [  91  698    1]\n",
      " [  20   15   37]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95      1698\n",
      "           1       0.89      0.88      0.89       790\n",
      "           2       0.93      0.51      0.66        72\n",
      "\n",
      "    accuracy                           0.92      2560\n",
      "   macro avg       0.92      0.78      0.83      2560\n",
      "weighted avg       0.92      0.92      0.92      2560\n",
      "\n",
      "0.920703125\n",
      "\n",
      "morph model\n",
      "[[1634   61    3]\n",
      " [ 100  690    0]\n",
      " [  17   18   37]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.96      0.95      1698\n",
      "           1       0.90      0.87      0.89       790\n",
      "           2       0.93      0.51      0.66        72\n",
      "\n",
      "    accuracy                           0.92      2560\n",
      "   macro avg       0.92      0.78      0.83      2560\n",
      "weighted avg       0.92      0.92      0.92      2560\n",
      "\n",
      "0.922265625\n",
      "token_eng model\n",
      "[[1643   48    7]\n",
      " [ 102  684    4]\n",
      " [  16   15   41]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.97      0.95      1698\n",
      "           1       0.92      0.87      0.89       790\n",
      "           2       0.79      0.57      0.66        72\n",
      "\n",
      "    accuracy                           0.93      2560\n",
      "   macro avg       0.88      0.80      0.83      2560\n",
      "weighted avg       0.92      0.93      0.92      2560\n",
      "\n",
      "0.925\n"
     ]
    }
   ],
   "source": [
    "print(\"token model\")\n",
    "print(confusion_matrix(y_token_test, token_predictions))  \n",
    "print(classification_report(y_token_test, token_predictions))  \n",
    "print(accuracy_score(y_token_test, token_predictions))\n",
    "\n",
    "print(\"\\nmorph model\")\n",
    "print(confusion_matrix(y_morph_test, morph_predictions))  \n",
    "print(classification_report(y_morph_test, morph_predictions))  \n",
    "print(accuracy_score(y_morph_test, morph_predictions))\n",
    "\n",
    "print(\"token_eng model\")\n",
    "print(confusion_matrix(y_token_test_eng, token_eng_predictions))  \n",
    "print(classification_report(y_token_test_eng, token_eng_predictions))  \n",
    "print(accuracy_score(y_token_test_eng, token_eng_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11db0006",
   "metadata": {},
   "source": [
    "We can see the diffrences between the features of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a8699519",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf_token_converter.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97a9ae35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf_morph_converter.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3fab9508",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf_token_eng_converter.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c7de7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4c1b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ce7b72d",
   "metadata": {},
   "source": [
    "## SECOND MODEL\n",
    "Using SVM classifier based on TD-IDF and POS Tagger\n",
    "I will use only the english dataset, so i will be able to use nltk pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91160aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_eng_text_series function will get text Serier and clean it with regex expressions\n",
    "def process_eng_text_series(series):\n",
    "\n",
    "    for i,text in enumerate(series):  \n",
    "        # Remove all the special characters\n",
    "        text = re.sub(r'\\W', ' ', str(text))\n",
    "\n",
    "        # Remove all digits\n",
    "        #processed_text = re.sub(r'\\d+', ' ', processed_text)\n",
    "        \n",
    "        # remove all single characters\n",
    "        text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
    "\n",
    "        # Remove single characters from the start\n",
    "        text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text) \n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        text= re.sub(r'\\s+', ' ', text, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        text = re.sub(r'^b\\s+', '', text)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        series[i] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91ff2dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put labels on the coloumns\n",
    "token_train_eng = token_train_eng.rename(columns = {0: 'text', 1: 'label'}, inplace = False)\n",
    "token_test_eng = token_test_eng.rename(columns = {0: 'text', 1: 'label'}, inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f45c0d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-6b885cfa3a50>:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  series[i] = text\n"
     ]
    }
   ],
   "source": [
    "#Clean dataset using process_eng_text_series function\n",
    "process_eng_text_series(token_train_eng['text'])\n",
    "process_eng_text_series(token_test_eng['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e06e2e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_tokenize will get text and return it tokenized in list\n",
    "token_train_eng['text']= [word_tokenize(entry) for entry in token_train_eng['text']]\n",
    "token_test_eng['text']= [word_tokenize(entry) for entry in token_test_eng['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be34ad84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step - d : Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04b8df92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_lemmatization function will get dataset and lemmatize the text.\n",
    "#lemmatization is the process of converting a word to its base form.\n",
    "def data_lemmatization(data):\n",
    "    for index,entry in enumerate(data['text']):\n",
    "        Final_words = []\n",
    "        word_Lemmatized = WordNetLemmatizer()\n",
    "        for word, tag in pos_tag(entry):\n",
    "            if word not in stopwords.words('english') and word.isalpha():\n",
    "                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "                Final_words.append(word_Final)\n",
    "\n",
    "        data.loc[index,'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d23fe617",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatization(token_train_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d06baa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatization(token_test_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4ec1fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperate the text and the sentiment labels\n",
    "token_train_eng_X = token_train_eng['text_final']\n",
    "token_train_eng_Y = token_train_eng['label']\n",
    "token_test_eng_X = token_test_eng['text_final']\n",
    "token_test_eng_Y = token_test_eng['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3320e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_train_eng_Y = token_train_eng_Y.values\n",
    "token_test_eng_Y = token_test_eng_Y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72d69bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create feature vectors containing TF-IDF values\n",
    "Tfidf_vect_eng = TfidfVectorizer(max_features=2000)\n",
    "Tfidf_vect_eng.fit(token_train_eng['text_final'])\n",
    "token_train_eng_X_Tfidf = Tfidf_vect_eng.transform(token_train_eng_X)\n",
    "token_test_eng_X_Tfidf = Tfidf_vect_eng.transform(token_test_eng_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6512c68",
   "metadata": {},
   "source": [
    "## Build Classifier\n",
    "I will use SVM to classify the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ea6f91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_eng model\n",
      "[[1592   98    8]\n",
      " [ 160  624    6]\n",
      " [  28   21   23]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.94      0.92      1698\n",
      "           1       0.84      0.79      0.81       790\n",
      "           2       0.62      0.32      0.42        72\n",
      "\n",
      "    accuracy                           0.87      2560\n",
      "   macro avg       0.79      0.68      0.72      2560\n",
      "weighted avg       0.87      0.87      0.87      2560\n",
      "\n",
      "0.874609375\n"
     ]
    }
   ],
   "source": [
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(token_train_eng_X_Tfidf,token_train_eng_Y)\n",
    "\n",
    "predictions_SVM = SVM.predict(token_test_eng_X_Tfidf)\n",
    "\n",
    "print(\"token_eng model\")\n",
    "print(confusion_matrix(token_test_eng_Y, predictions_SVM))  \n",
    "print(classification_report(token_test_eng_Y, predictions_SVM))  \n",
    "print(accuracy_score(token_test_eng_Y, predictions_SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b6488f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad261d27",
   "metadata": {},
   "source": [
    "## With the Random Forest Classifier (FIRST MODEL) i achieved better results than the SVM classfier."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
